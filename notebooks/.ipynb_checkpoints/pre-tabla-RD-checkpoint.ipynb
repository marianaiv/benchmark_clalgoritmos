{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos simples de clasificación\n",
    "Para analizar los algoritmos del LHCO 2020 empezaremos implementando y caracterizando algoritmos simples de clasificación.\n",
    "\n",
    "# Pre-procesamiento\n",
    "Inicialmente debemos tabular las variables con las que los algoritmos podrían realizar la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías a utilizar\n",
    "import h5py                             # Para manejar los archivos .h5\n",
    "import numpy as np                      # Manejo de matrices\n",
    "import matplotlib.pyplot as plt         # Plots\n",
    "import pyjet as fj                      # Clustering de los jets\n",
    "import pandas as pd                     # Manejo de tablas\n",
    "import os.path                          # Manejo de directorios\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VER COMO USAR ESTO ##\n",
    "def generador(nombre_archivo, chunksize=512,total_size=1100000):\n",
    "\n",
    "    m = 0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        yield pd.read_hdf(nombre_archivo,start=m*chunksize, stop=(m+1)*chunksize)\n",
    "\n",
    "        m+=1\n",
    "        if (m+1)*chunksize > total_size:\n",
    "            m=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"../../events_anomalydetection.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eventos = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample = df.sample(n=n_eventos)\n",
    "\n",
    "# Los guardamos en un archivo .h5 para tener la muestra en la que se haga el análisis\n",
    "if path.exists(\"events_anomalydetection_tiny_{}.h5\".format(n_eventos))!= True: \n",
    "    dfsample.to_hdf(\"events_anomalydetection_tiny_{}.h5\".format(n_eventos), key='df', mode='w',complevel=5,complib='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eventos_tiny = pd.read_hdf(\"events_anomalydetection_tiny_{}.h5\".format(n_eventos))\n",
    "eventos_tiny = dfsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(eventos_tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para calcular variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltaR(x, y):\n",
    "    return ((x.phi-y.phi)**2 + (x.eta-y.eta)**2)**0.5\n",
    "\n",
    "def subjettiness(cndts, cnsts):\n",
    "    d0 = sum(c.pt for c in cnsts)\n",
    "    ls = []\n",
    "    for c in cnsts:\n",
    "        dRs = [deltaR(c,cd) for cd in cndts]\n",
    "        ls += [c.pt * min(dRs)]\n",
    "    return sum(ls)/d0\n",
    "\n",
    "def tau21(jet,subR=0.2):\n",
    "    '''Input: jet from the jet clustering result '''\n",
    "    jet_substruct_features = {}        \n",
    "    seq = fj.cluster(jet, R=subR, algo='kt')\n",
    "    cnsts = jet.constituents()\n",
    "    cndts1 = seq.exclusive_jets(1)\n",
    "    tau1 = subjettiness(cndts1, cnsts)\n",
    "    if (len(cnsts)>1):\n",
    "        cndts2 = seq.exclusive_jets(2)\n",
    "        tau2 = subjettiness(cndts2, cnsts)\n",
    "    else: \n",
    "        tau2 = 0\n",
    "        \n",
    "    if tau1 == 0: return 0    \n",
    "    else: return tau2/tau1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabla(datos):\n",
    "#senal = eventos_tiny.loc[eventos_tiny.iloc[:,-1] == 1]\n",
    "#senal_ss = senal.iloc[:,:-1]\n",
    "    n_eventos = datos.shape[0]              # número de eventos (1000)\n",
    "    n_hadrones_gen = int((datos.shape[1])/3)  # número de hadrones (700) \n",
    "                                                   # [-1 para eliminar la columna señal, /3 por las 3 caracteristicas de cada hadron]\n",
    "\n",
    "    df = pd.DataFrame(columns=['pT_j1', 'm_j1', 'eta_j1', 'phi_j1', 'E_j1', 'tau_21_j1',  \n",
    "                                'pT_j2', 'm_j2', 'eta_j2', 'phi_j2', 'E_j2', 'tau_21_j2',\n",
    "                                'm_jj', 'deltaR_j12'])\n",
    "\n",
    "    for evento in range(n_eventos):\n",
    "        if (evento%1000==0):\n",
    "                print(evento)\n",
    "                pass\n",
    "\n",
    "        pseudojets_input = np.zeros(len([data for data in datos.iloc[evento,::3] if data > 0]), dtype= fj.DTYPE_PTEPM) \n",
    "\n",
    "        for hadron in range(n_hadrones_gen):\n",
    "            if (datos.iloc[evento,hadron*3] > 0): ## si pT > 0 \n",
    "\n",
    "                ## Llenamos el arreglo con pT, eta y phi de cada \"partícula\"\n",
    "                pseudojets_input[hadron]['pT'] = datos.iloc[evento,hadron*3] \n",
    "                pseudojets_input[hadron]['eta'] = datos.iloc[evento,hadron*3+1]\n",
    "                pseudojets_input[hadron]['phi'] = datos.iloc[evento,hadron*3+2]\n",
    "\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        ## Devuelve una \"ClusterSequence\" (un tipo de lista de pyjet)\n",
    "        ## ** No sé en verdad que está haciendo la función. clustering con anti-kt? o con que? que es p? ## \n",
    "        secuencia = fj.cluster(pseudojets_input, R=1.0, p=-1) \n",
    "\n",
    "        ## Con inclusive_jets accedemos a todos los jets que fueron clusterizados\n",
    "        ## y filtramos los que tienen pT mayor que 20fj.\n",
    "        ## Hacemos una lista con objetos PseudoJet\n",
    "        jets = secuencia.inclusive_jets(ptmin=20) \n",
    "\n",
    "        # Agrega el pT, eta y phi del jet principal\n",
    "        # ** No sé cómo están en orden **\n",
    "        pT_j1 = jets[0].pt\n",
    "        m_j1 = np.abs(jets[0].mass)\n",
    "        eta_j1 = jets[0].eta\n",
    "        phi_j1 = jets[0].phi\n",
    "        E_j1 = jets[0].e\n",
    "        tau_21_j1= tau21(jets[0])\n",
    "\n",
    "        try:\n",
    "            pT_j2 = jets[1].pt\n",
    "            m_j2 = np.abs(jets[1].mass)\n",
    "            eta_j2 = jets[1].eta\n",
    "            phi_j2 = jets[1].phi\n",
    "            E_j2 = jets[1].e\n",
    "            tau_21_j2= tau21(jets[1])\n",
    "        except IndexError:\n",
    "            pT_j2 = 0\n",
    "            m_j2 = 0\n",
    "            eta_j2 = 0\n",
    "            phi_j2 = 0\n",
    "            E_j2 = 0\n",
    "            tau_21_j2 = 0\n",
    "\n",
    "        deltaR_j12 = deltaR(jets[0], jets[1])\n",
    "        mjj = m_j1 + m_j2\n",
    "        n_hadrones = senal_ss.iloc[evento,:].astype(bool).sum(axis=0)/3\n",
    "\n",
    "        entry = pd.DataFrame([[pT_j1, m_j1, eta_j1, phi_j1, E_j1, tau_21_j1,  \n",
    "                                pT_j2, m_j2, eta_j2, phi_j2, E_j2, tau_21_j2, \n",
    "                                mjj,deltaR_j12, n_hadrones]],\n",
    "                            columns=['pT_j1', 'm_j1', 'eta_j1', 'phi_j1', 'E_j1', 'tau_21_j1',  \n",
    "                                    'pT_j2', 'm_j2', 'eta_j2', 'phi_j2', 'E_j2', 'tau_21_j2',\n",
    "                                    'm_jj', 'deltaR_j12', 'n_hadrones'])\n",
    "        df = df.append(entry)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senal = eventos_tiny.loc[eventos_tiny.iloc[:,-1] == 1]\n",
    "senal_ss = senal.iloc[:,:-1]\n",
    "df_senal = tabla(senal_ss)\n",
    "df_senal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fondo = eventos_tiny.loc[eventos_tiny.iloc[:,-1] == 0]\n",
    "fondo_ss = fondo.iloc[:,:-1]\n",
    "df_fondo = tabla(fondo_ss)\n",
    "df_fondo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
