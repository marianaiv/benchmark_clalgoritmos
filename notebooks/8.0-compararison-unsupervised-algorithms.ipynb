{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2bc153",
   "metadata": {},
   "source": [
    "# Algorithms comparison - Unsupervised\n",
    "In this notebook I am going to use the pre-processed data to train unsupervised algorithms. I will compare its performance using some numerical metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd98142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f0169",
   "metadata": {},
   "source": [
    "## Data\n",
    "I will import the data and separate it for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c513ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pT_j1</th>\n",
       "      <th>m_j1</th>\n",
       "      <th>eta_j1</th>\n",
       "      <th>phi_j1</th>\n",
       "      <th>E_j1</th>\n",
       "      <th>tau_21_j1</th>\n",
       "      <th>nhadrons_j1</th>\n",
       "      <th>pT_j2</th>\n",
       "      <th>m_j2</th>\n",
       "      <th>eta_j2</th>\n",
       "      <th>phi_j2</th>\n",
       "      <th>E_j2</th>\n",
       "      <th>tau_21_j2</th>\n",
       "      <th>nhadrons_j2</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>deltaR_j12</th>\n",
       "      <th>n_hadrons</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1286.727685</td>\n",
       "      <td>106.912129</td>\n",
       "      <td>0.185508</td>\n",
       "      <td>-2.763676</td>\n",
       "      <td>1313.290435</td>\n",
       "      <td>0.624659</td>\n",
       "      <td>36</td>\n",
       "      <td>1283.220733</td>\n",
       "      <td>63.164215</td>\n",
       "      <td>0.064989</td>\n",
       "      <td>0.393688</td>\n",
       "      <td>1287.481934</td>\n",
       "      <td>0.713248</td>\n",
       "      <td>33</td>\n",
       "      <td>2580.489568</td>\n",
       "      <td>3.159663</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1354.394070</td>\n",
       "      <td>614.269108</td>\n",
       "      <td>0.826505</td>\n",
       "      <td>1.365524</td>\n",
       "      <td>1943.559886</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>84</td>\n",
       "      <td>1325.613761</td>\n",
       "      <td>439.064150</td>\n",
       "      <td>-0.874319</td>\n",
       "      <td>-1.786248</td>\n",
       "      <td>1916.370744</td>\n",
       "      <td>0.276881</td>\n",
       "      <td>97</td>\n",
       "      <td>3859.315047</td>\n",
       "      <td>3.581406</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1214.955723</td>\n",
       "      <td>645.865619</td>\n",
       "      <td>-0.196786</td>\n",
       "      <td>2.040545</td>\n",
       "      <td>1396.840654</td>\n",
       "      <td>0.238205</td>\n",
       "      <td>119</td>\n",
       "      <td>1072.462085</td>\n",
       "      <td>113.768840</td>\n",
       "      <td>0.143831</td>\n",
       "      <td>-1.090330</td>\n",
       "      <td>1089.530630</td>\n",
       "      <td>0.726963</td>\n",
       "      <td>59</td>\n",
       "      <td>2480.769725</td>\n",
       "      <td>3.149348</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1285.227873</td>\n",
       "      <td>516.835248</td>\n",
       "      <td>0.328693</td>\n",
       "      <td>2.975321</td>\n",
       "      <td>1450.485926</td>\n",
       "      <td>0.013429</td>\n",
       "      <td>65</td>\n",
       "      <td>1220.251279</td>\n",
       "      <td>174.796077</td>\n",
       "      <td>0.294854</td>\n",
       "      <td>-0.322661</td>\n",
       "      <td>1285.618789</td>\n",
       "      <td>0.706361</td>\n",
       "      <td>89</td>\n",
       "      <td>2609.893413</td>\n",
       "      <td>3.298155</td>\n",
       "      <td>183.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1210.415787</td>\n",
       "      <td>129.499352</td>\n",
       "      <td>-0.744836</td>\n",
       "      <td>-2.883347</td>\n",
       "      <td>1567.345300</td>\n",
       "      <td>0.423550</td>\n",
       "      <td>54</td>\n",
       "      <td>1091.785816</td>\n",
       "      <td>155.362262</td>\n",
       "      <td>1.060534</td>\n",
       "      <td>0.264977</td>\n",
       "      <td>1772.340209</td>\n",
       "      <td>0.787662</td>\n",
       "      <td>57</td>\n",
       "      <td>3313.488835</td>\n",
       "      <td>3.629229</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pT_j1        m_j1    eta_j1    phi_j1         E_j1  tau_21_j1  \\\n",
       "0  1286.727685  106.912129  0.185508 -2.763676  1313.290435   0.624659   \n",
       "1  1354.394070  614.269108  0.826505  1.365524  1943.559886   0.311688   \n",
       "2  1214.955723  645.865619 -0.196786  2.040545  1396.840654   0.238205   \n",
       "3  1285.227873  516.835248  0.328693  2.975321  1450.485926   0.013429   \n",
       "4  1210.415787  129.499352 -0.744836 -2.883347  1567.345300   0.423550   \n",
       "\n",
       "   nhadrons_j1        pT_j2        m_j2    eta_j2    phi_j2         E_j2  \\\n",
       "0           36  1283.220733   63.164215  0.064989  0.393688  1287.481934   \n",
       "1           84  1325.613761  439.064150 -0.874319 -1.786248  1916.370744   \n",
       "2          119  1072.462085  113.768840  0.143831 -1.090330  1089.530630   \n",
       "3           65  1220.251279  174.796077  0.294854 -0.322661  1285.618789   \n",
       "4           54  1091.785816  155.362262  1.060534  0.264977  1772.340209   \n",
       "\n",
       "   tau_21_j2  nhadrons_j2         m_jj  deltaR_j12  n_hadrons  label  \n",
       "0   0.713248           33  2580.489568    3.159663      109.0    0.0  \n",
       "1   0.276881           97  3859.315047    3.581406      208.0    0.0  \n",
       "2   0.726963           59  2480.769725    3.149348      196.0    0.0  \n",
       "3   0.706361           89  2609.893413    3.298155      183.0    0.0  \n",
       "4   0.787662           57  3313.488835    3.629229      169.0    1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data \n",
    "from benchtools.src.datatools import read_multifiles\n",
    "\n",
    "df = read_multifiles(filename='RD_dataset', nbatch=10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5cab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from benchtools.src.datatools import separate_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separating variables from label\n",
    "X, y = separate_data(df, standardize=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# Eliminating the columns of mass so that the training is model-free \n",
    "X_train_nm = X_train.drop(['m_j1', 'm_j2', 'm_jj'], axis=1)\n",
    "X_test_nm = X_test.drop(['m_j1', 'm_j2', 'm_jj'], axis=1)\n",
    "X_nm = X.drop(['m_j1', 'm_j2', 'm_jj'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb3d07",
   "metadata": {},
   "source": [
    "## Classification\n",
    "I will use three different classificators and will calculate the following metrics for each:\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Rand index\n",
    "\n",
    "According to [sklearn site](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation):\n",
    "\n",
    "> Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric.\n",
    "\n",
    "The rand index is a measure of the similarity between two data clusterings. We will use it to address this. However, we will calculate the other variables because we want to compare these algorithms with the supervised ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817fe21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the metrics\n",
    "from sklearn.metrics import precision_score, log_loss, recall_score, plot_confusion_matrix,classification_report, f1_score, silhouette_score, rand_score\n",
    "\n",
    "# I will define a function that calculates all the metrics and returns a dataframe with them \n",
    "def metrics(log, log_cols, name, y_test, y_pred, X_pca_train):\n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "\n",
    "    print('****Results****')\n",
    "\n",
    "    # Calculating metrics\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    rand = rand_score(y_test, y_pred)\n",
    "\n",
    "    # Prin the report\n",
    "    print(classification_report(y_test, y_pred, target_names=['background','signal']))\n",
    "\n",
    "    # Inserting the data in the dataframe\n",
    "    log_entry = pd.DataFrame([[name, recall, precision*100, f1, rand]], columns=log_cols)\n",
    "    return log_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3f1dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KMeans\n",
      "****Results****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.91      0.50      0.64    139578\n",
      "      signal       0.09      0.50      0.15     14022\n",
      "\n",
      "    accuracy                           0.50    153600\n",
      "   macro avg       0.50      0.50      0.40    153600\n",
      "weighted avg       0.83      0.50      0.60    153600\n",
      "\n",
      "==============================\n",
      "Birch\n",
      "****Results****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.91      0.50      0.64    139578\n",
      "      signal       0.09      0.50      0.15     14022\n",
      "\n",
      "    accuracy                           0.50    153600\n",
      "   macro avg       0.50      0.50      0.40    153600\n",
      "weighted avg       0.83      0.50      0.60    153600\n",
      "\n",
      "==============================\n",
      "GaussianMixture\n",
      "****Results****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.91      0.50      0.64    139578\n",
      "      signal       0.09      0.50      0.15     14022\n",
      "\n",
      "    accuracy                           0.50    153600\n",
      "   macro avg       0.50      0.50      0.40    153600\n",
      "weighted avg       0.83      0.50      0.60    153600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the algorithms\n",
    "from sklearn.cluster import KMeans, Birch, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Record for visual comparison \n",
    "log_cols=[\"Classifier\", \"Precision\", \"Recall\", \"F1 score\", \"Rand score\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "classifiers = [KMeans(n_clusters=2, random_state=19)\n",
    "              , Birch(n_clusters=2)\n",
    "              , GaussianMixture(n_components=2, random_state=0)\n",
    "              ]\n",
    "\n",
    "for clf in classifiers:\n",
    "    try: del y_pred\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Getting the name\n",
    "    name = clf.__class__.__name__\n",
    "    # Training\n",
    "    clf.fit(X_train_nm)\n",
    "    # Obtaining predictions\n",
    "    y_pred = clf.predict(X_test_nm)\n",
    "    # Obtaining metrics\n",
    "    log_entry = metrics(log, log_cols, name, y_test, y_pred, X_train_nm)\n",
    "    log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449866f7",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "Here I'll plot the values to compare the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa257a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Defining a function for ploting the bar plots\n",
    "def comparative_barplot(log, x, color):\n",
    "    sns.set_color_codes(\"muted\")\n",
    "    sns.barplot(x=x, y='Classifier', data=log, color=color)\n",
    "    plt.xlabel('{}'.format(x))\n",
    "    plt.title('Classifiers: {}'.format(x))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d63b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Rand score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>0.495222</td>\n",
       "      <td>9.020408</td>\n",
       "      <td>0.15261</td>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birch</td>\n",
       "      <td>0.495222</td>\n",
       "      <td>9.020408</td>\n",
       "      <td>0.15261</td>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianMixture</td>\n",
       "      <td>0.495222</td>\n",
       "      <td>9.020408</td>\n",
       "      <td>0.15261</td>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Classifier  Precision    Recall  F1 score  Rand score\n",
       "0           KMeans   0.495222  9.020408   0.15261    0.500005\n",
       "0            Birch   0.495222  9.020408   0.15261    0.500005\n",
       "0  GaussianMixture   0.495222  9.020408   0.15261    0.500005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = ['b', 'y', 'r', 'g', 'o']\n",
    "columns = log.columns.tolist()\n",
    "columns.remove('Classifier')\n",
    "log.head()\n",
    "#for column,color in zip(columns, colors):\n",
    "#    comparative_barplot(log, column, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7de77",
   "metadata": {},
   "source": [
    "In general, the algorithms did not performed well. The precision, f1 scores and rand index are low. Also, we see that we can't select any of them as better seems all of them seem to have about the same metrics. Checking the classification report above, it can be seen that the classification is indeed really similar.\n",
    "\n",
    "## Reduction of dimensions\n",
    "I'll try reducing the dimension of the input to see if the algorithms improve the classification. Principal component analysis, or PCA, is commonly use for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac676bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Reducing to 5 inputs\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_train_nm)\n",
    "\n",
    "# Transforming the data to this inputs\n",
    "X_pca_train = pca.transform(X_train_nm)\n",
    "X_pca_train = pd.DataFrame(X_pca_train)\n",
    "\n",
    "X_pca_test = pca.transform(X_test_nm)\n",
    "X_pca_test = pd.DataFrame(X_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19364f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KMeans\n",
      "****Results****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.91      0.50      0.64    139578\n",
      "      signal       0.09      0.50      0.15     14022\n",
      "\n",
      "    accuracy                           0.50    153600\n",
      "   macro avg       0.50      0.50      0.40    153600\n",
      "weighted avg       0.83      0.50      0.60    153600\n",
      "\n",
      "==============================\n",
      "Birch\n",
      "****Results****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.91      0.50      0.65    139578\n",
      "      signal       0.09      0.50      0.16     14022\n",
      "\n",
      "    accuracy                           0.50    153600\n",
      "   macro avg       0.50      0.50      0.40    153600\n",
      "weighted avg       0.84      0.50      0.60    153600\n",
      "\n",
      "==============================\n",
      "GaussianMixture\n",
      "****Results****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  background       0.91      0.50      0.65    139578\n",
      "      signal       0.09      0.50      0.16     14022\n",
      "\n",
      "    accuracy                           0.50    153600\n",
      "   macro avg       0.50      0.50      0.40    153600\n",
      "weighted avg       0.84      0.50      0.60    153600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record for visual comparison \n",
    "log_cols=[\"Classifier\", \"Precision\", \"Recall\", \"F1 score\", \"Rand score\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "classifiers = [KMeans(n_clusters=2, random_state=15)\n",
    "              , Birch(n_clusters=2)\n",
    "              , GaussianMixture(n_components=2, random_state=0)\n",
    "              ]\n",
    "\n",
    "for clf in classifiers:\n",
    "    # Getting the name\n",
    "    name = clf.__class__.__name__\n",
    "    # Training\n",
    "    clf.fit(X_pca_train)\n",
    "    # Obtaining predictions\n",
    "    y_pred = clf.predict(X_pca_test)\n",
    "    # Obtaining metrics\n",
    "    log_entry = metrics(log, log_cols, name, y_test, y_pred, X_pca_train)\n",
    "    log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f1f06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Rand score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>0.495222</td>\n",
       "      <td>9.020408</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birch</td>\n",
       "      <td>0.504778</td>\n",
       "      <td>9.237917</td>\n",
       "      <td>0.156177</td>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianMixture</td>\n",
       "      <td>0.504778</td>\n",
       "      <td>9.237917</td>\n",
       "      <td>0.156177</td>\n",
       "      <td>0.500005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Classifier  Precision    Recall  F1 score  Rand score\n",
       "0           KMeans   0.495222  9.020408  0.152610    0.500005\n",
       "0            Birch   0.504778  9.237917  0.156177    0.500005\n",
       "0  GaussianMixture   0.504778  9.237917  0.156177    0.500005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = ['b', 'y', 'r', 'g', 'o']\n",
    "columns = log.columns.tolist()\n",
    "columns.remove('Classifier')\n",
    "\n",
    "log.head()\n",
    "#for column,color in zip(columns, colors):\n",
    "#    comparative_barplot(log, column, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ddf89",
   "metadata": {},
   "source": [
    "Although the classification did not improve, we can see a difference particularly in KMeans. Surprisingly, the precision, recall and f1 score decreased for this classificator. Further exploration has to be done to see how are these classifiers classifying."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
